% You should title the file with a .tex extension (hw1.tex, for example)
\documentclass[11pt]{article}

\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{listings}
\usepackage{algpseudocode}
\usepackage{color}
\usepackage{color}   %May be necessary if you want to color links
\usepackage[colorlinks]{hyperref}


\oddsidemargin0cm
\topmargin-2cm     %I recommend adding these three lines to increase the 
\textwidth16.5cm   %amount of usable space on the page (and save trees)
\textheight23.5cm  




\begin{document}
\tableofcontents

\clearpage

\section{Machine Learning Principles}

There are three main divisions of Machine Learning.

\begin{itemize}
\item Supervised Learning - Given a labeled dataset, learn from it and apply it to a new unlabeled dataset (ie identify the function or approximation that produces the proper label).
\item Unsupervised Learning - Creating structure from inputs that have no labels
\item Reinforcement Learning - Getting feedback on actions over time to learn from.
\end{itemize}

The goal of machine learning is to (loosely) generalize information about a particular system. It's a form of induction (ie. rules are created based on data) as opposed to deduction (applying a rule to a datapoint). While each division above is defined differently, they can be combined together. For example, we can use unsupervised learning to summarize the data (ie provide a set of labels), and then run supervised learning to label new data points accordingly.

\section{Key Terms}

\par\noindent\hangindent=1cm \textbf{Classification} A mapping of inputs to a discrete label\\

\par\noindent\hangindent=1cm \textbf{Cross Validation} Dividing the training sets randomly into a smaller training set and a validation set, running training on the smaller testing set and run it on the validation set. Applying this multiple times can increase potential accuracy on the final test set\\

\par\noindent\hangindent=1cm \textbf{Inductive Bias} Set of assumptions that, when combined with the training data, describe the classifications assigned by the learner to future data instances.\\

\par\noindent\hangindent=1cm \textbf{Overfitting} Essentially tuning your algorithm too closely to your training dataset that it does worse in general on the testing set. Mathematically speaking, suppose we took the set of all the possible hypotheses that could explain the data $H$, and we propose a hypothesis $h$. If there's an alternate hypothesis (let's call it $h'$) that performs better on the test data (ie has a smaller error), our hypothesis is said to overfit. This can be the case even if our hypothesis performs better on our training set.\\

\par\noindent\hangindent=1cm \textbf{Regression} A mapping of continuous inputs to outputs (ie approximating a function to produce a continuous value)\\

\par\noindent\hangindent=1cm \textbf{Squared Error} A mapping of continuous inputs to outputs (ie approximating a function to produce a continuous value)\\

\subsection{Regression and Classification}
\begin{itemize}
\item Least squared error:The objective consists of adjusting the parameters of a model function to best fit a data set. A simple data set consists of n points (data pairs) $(x_i,y_i)$, i = 1, ..., n, where $x_i$ is an independent variable and $y_i$ is a dependent variable whose value is found by observation. The model function has the form $f(x,\beta)$, where the m adjustable parameters are held in the vector $\boldsymbol \beta$. The goal is to find the parameter values for the model which "best" fits the data. The least squares method finds its optimum when the sum, S, of squared residuals

$S=\sum_{i=1}^{n}{r_i}^2$
is a minimum. A residual is defined as the difference between the actual value of the dependent variable and the value predicted by the model.

$r_i=y_i-f(x_i,\boldsymbol \beta)$
An example of a model is that of the straight line in two dimensions. Denoting the intercept as $\beta_0$ and the slope as $\beta_1$, the model function is given by $f(x,\boldsymbol \beta)=\beta_0+\beta_1 x$.
\end{itemize}

\section{Supervised Learning}

\subsection{Decision Trees}

Decision trees allows the ability generate a function that produces a discrete value output. Decision trees lend themselves easily to be written as a series of if-then statements in code. This is because decision trees can be written in disjunction of conjunctions. For example, each branching at a feature can be considered an OR statement, and each parent child relationship is an AND statement. From the Mitchell's textbook, they describe it well with the following statement.

We will play tennis IF (the outlook is sunny AND the humidity is normal) OR (the outlook is overcast) OR (the outlook is rain AND the wind is weak).

\subsubsection{Good Problems}

The reasons below are general guidelines that indicate Decision Trees are valuable. 

\begin{itemize}
\item There are a fix set of features with discrete possible values (though extensions have been implemented to allow continuous values for features). For example, saying the temperature is hot, warm, or cold, is more discrete than saying the temperature is in the range from 30 degrees to 100 degrees.
\item The problem is a classification problem. Decision trees in the end produce a value (eg. whether to go outside or not). They tend to be not good at providing a real-value (eg. What is the overall ranking this person will receive).
\item The description of the approximation needs to be defined (usually in a disjunctive format).
\item Training data may contain errors - Decision trees have mechanisms to work around missing data and errors in classification in general.
\end{itemize}

\subsubsection{Implementation Details}

Implementation involving what attribute to branch off and in what order. The most common choice here is the ID3 algorithm.

\subsubsection*{ID3 Algorithm}

The ID3 algorithm involves looking at the statistic known as \textit{information gain}. At a high level, a good attribute to branch off of is an attribute that roughly can divide equally between the discrete options (for example half of the dataset. In addition, we'd like the division to be reasonably valid (ie it classifies most accurately). Another measure from Information Theory called \textit{Entropy} is used. Entropy defines a measure on how impure the data is. For a dataset $S$ combined with the proportion of correct attribute values, $p_i$ for $c$ possible attribute values, we can define Entropy as follows.

\begin{equation}
Entropy(S) = \sum_{i=1}^{c} -p_i \log_2 p_i
\end{equation}

As an edge case, we define $\log_2 0$ to be $0$. This will generate a curve that prefers even splits among the attribute values. If all of the answers are correct and all are incorrect, no new information is obtained.

We use Entropy to define \textit{Information Gain} $Gain(S, A)$ over a dataset $S$ and an attribute $A$ as a definition of how much entropy is removed when splitting on that attribute.

\begin{equation}
Gain(S,A) = Entropy(S) - \sum_{v \in values(A)} \frac{|S_v|}{|S|} Entropy(S_v)
\end{equation}

where $Values(A)$ is the set of all possible values an attribute $A$ could have and $S_v$ is the set of all values, $s \in S$ where that attribute $A$ is set to $v$ (Also described as $A(s) = v$).

\subsubsection*{Bias}

Inductive Bias is to prefer shorter trees over longer trees and favor trees that place high information gain closer to the root. This is an example of a \textit{Preference Bias} where it does not explicitly eliminate the hypothesis space but assigns a value to each hypothesis and chooses the best one. In contrast, a \textit{Restriction Bias} rules out hypotheses explicitly and is never considered again.

\subsubsection{Risks}

\begin{itemize}
\item Decision trees use an inductive learning method by searching through the hypothesis space. It performs a hill climbing algorithm (though could be changed nowadays) which leads to potential local optimums.
\item Decision trees use a greedy algorithm which lends to an inability to backtrack
\item Decision can lead to overfitting. This can be resolved by applying a pruning process either by restricting the tree height to begin with or removing nodes after an overfitted tree (better in practice).
\end{itemize}

There are two main types of pruning: Reduced Error Pruning and Rule-Post Pruning. Reduced Error Pruning consists of when removing a subtree produces the same, if not better error rate on the test data. Rule Post-Pruning involves converting an overfitted tree into its set of rules (disjunction of conjunctions), removing the conditions and evaluating the rule. Remove the rule if doing so performs no worse than before. Using a Cross-Validation is useful for this case.

\subsubsection{Variants}

\subsubsection*{Continuous Value Attributes}

To include continuous values for an attribute, divide the attribute up into discrete intervals. The best way to consider is to compare when the classification boundary line changes when mapping the attribute value to its classification.

\subsubsection*{Alternate Measurements}

Instead of ID3, we can use the GainRatio which penalizes a value based on its split information (ie how many discrete values there are). This measurement can be useful when fields are have a large number of possible values (like dates). The equations are provided below.

\begin{equation}
SplitInformation(S,A) = -\sum_{i=1}^{c} \frac{|S_i|}{|S|} \log_2(\frac{|S_i|}{|S|})
\end{equation}

\begin{equation}
GainRatio(S,A) = \frac{Gain(S, A)}{SplitInformation(S, A)}
\end{equation}

\subsubsection*{Missing Attribute Values}

Examples with missing attribute values can be handled easily in Decision Trees. There are multiple strategies like assigning them the most common value among the data in that tree node. More complex is to employ probability to derive the likelihood the value is one over another. An updated algorithm C4.5 handles this case and more.

\subsubsection*{Higher Cost Attributes}

The $InformationGain$ function can be adjusted in terms of the cost of an attribute. An example here is a medical diagnosis decision tree where it may reduce the value for an invasive surgery to be less desirable though informational than say a blood test with some failure rate. Some values are described noting that they do not guarantee the optimal tree. Examples include:

\begin{equation}
\frac{Gain^2(S, A)}{Cost(A)}
\end{equation}

\begin{equation}
\frac{2^{Gain(S, A)} - 1}{(Cost(A) + 1)^w}
\end{equation}

where $w \in [0, 1]$ is a constant to determine the relative importance.

\subsection{Neural Networks}

Originally motivated from Biology with the neural system, neural networks have evolved to very loosely model a series of neurons.

\subsubsection{Good Problems}

The reasons below are general guidelines that indicate Neural Networks are valuable. 

\begin{itemize}
\item There are a large set of features to be learned. Examples such as pixels can help understand correlated or independent features more easily.
\item The problem is a classification, regression, or a combination of both. Neural Networks are very versatile in this manner.
\item An allowance of training upfront - Neural Networks require longer training times usually, especially in comparison of decision tree learning.
\item A quick evaluation of the target function - The advantage from the upfront training is a robust and much quicker evaluation function.
\item Training data may contain errors - Neural Networks have mechanisms to work around noisy data and errors in classification in general.
\item Description of approximation need not need definition - in contrast to decision tree, the ability to explain ``how" the algorithm interprets the input is not needed.
\end{itemize}

\subsubsection{Perceptrons}

The base unit that are ``networked" together in a Neural Network is a Perceptron. A perceptron takes in a vector (or collection) of inputs and evaluates piecewise to a discrete set of two values (positive or negative case). Each input is weighted and it must exceed a threshold to activate or emit a positive result. In the book, positive and negative results were 1 and -1 respectively whereas the video lectures used 1 and 0 respectively. Mathematically speaking, a perceptron can be defined as follows:

\begin{equation}
o(x_1...x_n) =
    \begin{cases}
            1, &         \text{if } w_0+w_1x_1+...+w_nx_n>0,\\
            0, &         \text{otherwise}.
    \end{cases}
\end{equation}

where $w_0,...,w_n$ is a real-valued weight. The videos do not include $w_0$ term, but they do introduce a threshold $\theta$. In this case $\theta = -w_0$ is a threshold that must be surpassed for the perceptron to output 1. Alternatively, we can use the $sgn$ function (outputting -1 or 1).

\begin{equation}
o(\vec{x}) = sgn(\vec{w}\vec{x}).
\end{equation}

In terms of hypothesis space, $H$, a perceptron can produce an infinite number of possibilities (equal to all possible real valued weight vectors).

\begin{equation}
H =\{\vec{w} | \vec{w} \in \mathbb{R}^{n+1} \}
\end{equation}

\subsubsection*{Logic Representation}

A perceptron can be described as representing many boolean functions like AND, OR, NAND, and NOR. A single perceptron can not, however, represent XOR. This leads to the final theorem that every boolean function can be represented by some network of perceptrons, which indicates the expressiveness of these. Example representations are included in the table below.

INSERT TABLE HERE

\subsubsection{Implementation Details}

Much of the implementation of a Neural Network requires a method of training each perceptron and setting the weights for each. One of the most effective methods to handle this is to use Gradient Descent.

\subsubsection*{Perceptron Training Rule}

The basic idea to train the perceptrons is to start with a random set of weights, apply it to a training datasets and update perceptron weights until it makes an error. The weights are then adjusted to work towards a valid solution. The process is iterated until the weights converge to set of values that work well on the entire training set. A weight is adjusted by adding some $\Delta$ value to it.

\begin{equation}
w_i \leftarrow w_i + \Delta w_i
\end{equation}

This adjustment can be calculated by learning a rate that moves closer to the correct classification.

\begin{equation}
\Delta w_i=\eta (t-o) x_i
\end{equation}

where $t$ is the target output for current training example (ie. the correct answer) and $o$ is the output generated for current training example (ie our wrong answer). The learning rate, represented by $\eta$ moves the weights into a direction to ensure that the weights are making smaller iterations towards the correct answer. It's usually set to some small value like $0.1$ and can be adjusted to decay as iterations increase.

Convergence occurs if the data is linearly separable (ie. there exists some hyperplane that separates classifications.

\subsubsection*{Delta Rule}

Because datasets are not linearly separable, there exists a problem with convergence that result in the algorithm not agreeing to halt. In this case, we can employ gradient descent and the delta rule. In this case, the training will converge to a ``best fit." A linear unit is used in the beginning (ie there is no threshold value $\theta$). In order to determine the best fit, a notion of an error value needs to be defined.

\begin{equation}
E(\vec{w})=\frac{1}{2} \sum_{d \in D} (t_d - o_d)^2
\end{equation}

where $D$ is our set of training examples, $t_d$ is the target output for the training example $d$ (a classification or regression), and $o_d$ is the output for the training example $d$.

Gradient descent finds a minimum by following the path of the hyperplane in the direction that points towards the minimum. Gradient Descent comes from calculus to be a vector of the partial derivative of the error with respect to each weight. The value of the gradient descent means the area where the error increases the greatest. This negation of the gradient descent therefore produces the greatest \textit{decrease} in $E$, the error. The method then follows the same process of updating the weights.

\begin{equation}
\vec{w} \leftarrow w + \Delta \vec{w}
\end{equation}

where we use the negative gradient descent to detemine the delta.

\begin{equation}
\Delta\vec{w} = -\eta \nabla E(\vec{w})
\end{equation}

where $\eta$ is the learning rate again. Separating it out for each component weight $w_i$.

\begin{equation}
\Delta w_i = -\eta \frac{\partial E}{\partial w_i}
\end{equation}

Calculation of the gradient descent is straightforward and easy to construct. To summarize, the updated change in weights per component  is:

\begin{equation}
\Delta w_i = \eta \sum_{d\in D} (t_d - o_d) x_{id}
\end{equation}

\subsubsection*{Stochastic Gradient Descent}

Gradient Descent has a couple challenges:

\begin{itemize}
\item It does not guarantee a global minimum it can get stuck at a local minimum.
\item It can be very slow to find that minimum requiring many iterations.
\end{itemize}

A solution is to apply a \textit{stochastic gradient descent} which updates the weights following each individual training example instead of all training examples (what gradient descent did). Note the lack of the summation in the updated error equation.

\begin{equation}
E_d(\vec{w})=\frac{1}{2} (t_d - o_d)^2
\end{equation}

There are three key differences:

\begin{itemize}
\item As stated before gradient descent is summed overall examples before updating weights. Stochastic gradient descent updates weights after each individual example.
\item It can be thought that gradient descent takes into account all training examples which is computationally expensive, but they can have a larger step size as a result. Stochastic gradient descent needs a smaller descent.
\item Although not guaranteed, stochastic gradient descent can avoid falling into a local minima because it uses different error gradients to guide the search.
\end{itemize}

\subsubsection{Multilayer networks}

Most of what we described above has been a focus on calculating weights for a single perceptron. Single perceptrons can only express linear decisions. Multilayer networks can express more complex decisions. The common method of training a multilayer network is through \textit{Backpropagation}. Multiple layers of linear units still produce linear results. Instead of the linear unit, we will use a differentiable threshold unit such as a sigmoid unit. It behaves very much like the discrete perceptron described above, but it provides a smoothed continuous curve to provide both scenarios.

\begin{equation}
o = \sigma (\vec{w} \vec{x})
\end{equation}
where
\begin{equation}
\sigma(y) = \frac{1}{1-e^y}
\end{equation}

The advantage of this scenario is that it is differentiable, which will make it easier to define the gradient descent.

\subsubsection*{\textsc{Backpropagation} Algorithm}

The \textsc{Backpropagation} algorithm learns the weights within a multilayer network (assuming the network is of fixed size and number of connections) through a similar gradient descent method to minimize the error. The redefined error function is below.

\begin{equation}
E(\vec{w}) = \frac12 \sum_{d\in D} \sum_{k \in outputs}{(t_{kd} - o_{kd})^2}
\end{equation}

where $outputs$ is the set of output units in the network, and $t_kd$ and $o_kd$ are the same target and output values but associated with the $k^{th}$ output unit and training example $d$. The algorithm behaves very similarly as before (feed weights to nodes, run it in the network, calculate error, and adjust weights based on gradient descent). One major difference in the algorithm is the update of weights relies on a more complex $\delta$ instead of the general error ($t-o$). The size of the network needs to be defined in order to run the algorithm, so various sizes will needed to be tested in order to identify the best output. Gradient descent will only guaranteed to converge to a local minima (but it often still produces better results).

\subsubsection{Risks}

\subsection{Momentum}
$$\Delta w^{(l)}_n= \eta \delta^{(l)}.*x^{(l)} +\alpha w^{(l)} (n-1) $$\\ where $n$ is the iteration (adds a momentum $\alpha$)
\begin{itemize}
\item $E_d(\vec{w}) = \frac12 \sum_{k\in outputs}{(t_k - o_k)^2}$ error on training example $d$
\item How to derive the \textsc{BACKPROP} rule??
\item \textsc{BACKPROP} for multi-layer networks may converge only at a local minimum (because error surface for multi-layer networks may contain many different minima).
\item Alternative Error Functions?
\item Alternative Error Minimization Procedures
\end{itemize}

\subparagraph{Recurrent Networks}
What do I need to know about recurrent networks?

\subsection{Radial Basis Functions}
\begin{itemize}
\item $\hat{f}(x) = w_0+ \sum_{u=1}^k{w_uKern_u(d(x_u, x))}$
\item Equation can be thought of as training a 2-layer network. First layer computes $Kern_u$, second layer computes a linear combination of these first layer values.
\item Kernel is defined such that $d(x_u, x) \uparrow \implies Kern_u \downarrow$
\item RBF gives global approximation to target function represented by linear combinations of many local kernel functions (smooth linear combination).
\item Faster to train than \textsc{BACKPROP} because input and output layer are trained separately.
\item RBF is eager: represents global function as a linear combo of multiple local kernel functions. Local approximations RBF creates are not specifically targeted to the query.
\item A type of ANN constructed from spatially localized kernel functions. Sort of the `link' between k-NN and ANN?
\end{itemize}


\section{Lazy vs Eager}
\begin{itemize}
\item k-NN, locally weighted regression, and case-based reasoning are lazy
\item \textsc{BACKPROP}, RBF is eager (why?), ID3 eager
\item Lazy algorithms may use query instance $x_q$ when deciding how to generalize (can represent as a bunch of local functions). Eager methods have already developed what they think is the global function.
\end{itemize}
\section{Instance Based Learning}
\subsection{k-NN}
\begin{itemize}
\item discrete: $$\hat{f} (x_q) =argmax_{v \in V} \sum_{i=1}^k{\delta(v, f(x_i))}$$ where $\delta(a,b) =1$ if $a=b$ and $0$ otherwise.
\item continuous (for a new value, $x_q$): $$\hat{f} (x_q) = \frac{\sum_{i=1}^{k}{f(x_i)}}{k}$$
\item distance-weighted: $w_i = \frac{1}{d(x_q, x_i)^2}$. If $x_q = x_i$ assign $\hat{f} (x_q) = f(x_i)$ (if more than one, do a majority).
\item real valued distance weighted: $$\hat{f} (x_q) = \frac{\sum_{i=1}^{k}{f(x_i)}}{\sum_{i=1}^{k}{w_i}}$$
\item  Inductive Bias of k-NN: assumption that nearest points are most similar
\item k-NN is sensitive to having many irrelevant attributes `curse of dimensionality' (can deal with it by `stretching the axes', add a weight to each attribute. Can even get rid of some of the attributes by setting the weight =0)
\end{itemize}

\subsection{Locally Weighted Linear Regression}
\begin{itemize}
\item $f$ approximated near $x_q$ using $\hat{f}(\vec{x}) = \vec{w} \cdot \vec{x}$ (is this appropriate notation?)
\item Error function using kernel: $E(x_q) = \frac12 \sum_{k\in K}{(f(x) - \hat{f}(x))^2 Kern(d(x_q, x))}$ where $K$ is the set of $k$ closest $x$ to $x_q$.
\end{itemize}

\section{Support Vector Machines}
Maximal Margin Hyperplanes: if data linearly separable, then $\exists (\vec{w}, b)$ such that $\vec{w}^T\vec{x} + b \geq 1$ $\forall \vec{x_i} \in P$ and $\vec{w}^T\vec{x} + b \leq -1$ $\forall \vec{x_i} \in N$ (N, P are the two classes). Want to minimize $\vec{w}^T\vec{w}$ subject to constraints of linear separability.\\ Or, maximize $\frac2{|w|}$ while $y_i(\vec{w}^T\vec{x_1}+b) \geq 1$ $\forall i$. Note $y_i =\{+1, -1\}$. Or minimize $\frac12 |w|^2$. This is quadratic programming problem.\\
$W(\alpha) = \sum_{i} \alpha_i - \frac12 \sum_{i,j} \alpha_i \alpha_j y_i y_j x_i^T x_j$. $w = \sum_i \alpha_i x_i y_i$. $\alpha_i$ mostly 0 $\implies$ only a few of the x's matter.
\subsection{Kernel Induced Feature Spaces}
Map to higher dimensional \textit{feature space}, construct a separating hyperplane. $X \rightarrow H$ is $\vec{x} \rightarrow \phi(\vec{x}).$\\
Decision function is $f(\vec{x}) = sgn (\phi(\vec{x}) w^*+b^*)$ (* means optimal weight and bias)\\
Kernel function: $K(\vec{x} \vec{z}) =\phi(\vec{x})^T\phi(\vec{z})$. If $K$ exists, we don't even need to know what $\phi$ is.\\
Mercer's condition: \\
What if data is not linearly separable? (slack variables?)\\
Lagrangian?\\
Mercer's Theorem? \\

\subsection{Relationship between SVMs and Boosting}
$H_{trial} (x) = \frac{sgn(\sum_i{\alpha_i x_i})}{\sum_i \alpha_i}$. As we use more and more weak learners, the error stays the same, but the confidence goes up. This equates to having a big margin (big margins tend to avoid overfitting). 


\section{Boosting}
Boosting problem: set of weak learners combined to produce a learner with an arbitrary high accuracy.\\
The original boosting problem asks whether a set
of weak learners can be combined to produce a learner with an arbitrary
high accuracy. A weak learner is a learner whose performance (at
classification or regression) is only slightly better than random guessing.
AdaBoost: trains multiple weak classifiers on training data, then combines into single boosted classifier. Weighted sum of weak classifiers with weights dependent on weak classifier accuracy.\\
$N$ training examples: $x_i$, $y_i \in \{-1, +1\}$. Each example $i$ has an observation weight $w_i$ (how important example $i$ is for our current learning task).\\
Classifier $G$: $err_S = \sum_{i=1}^N{w_i I(y_i \neq G(x_i))}$ \\
Using weights: $err = \frac{\sum_{i=1}^N{w_i I(y_i \neq G(x_i))}}{\sum_{i=1}^N w_i}$\\
In this way, our error metric is more sensitive to misclassified examples
that have a greater importance weight. Denominator is only for normalization (we want an answer between 0 and $N$).
Boosting: weights are sequentially updated. Algorithm:\\
\begin{itemize}
\item initialize $w_i = \frac1N$
\item for $m =1$ to $M$:
\begin{itemize}
\item fit $G_m(x)$ using $w_i$'s
\item compute $$err_m = \frac{\sum_{i=1}^N{w_i I(y_i \neq G_m(x_i))}}{\sum_{i=1}^N w_i}$$
\item $\alpha_m = \frac{log(1-err_m)}{err_m}$
\item $w_i \leftarrow w_i \cdot exp(\alpha_m I(y_i \neq G_m(x_i))$ for $i = 1 \dots N$
\end{itemize}
\item $G(x) = sign [ \sum_{m=1}^M \alpha_m G_m(x)]$ In this way, classifiers that have a poor accuracy (high error rate, low $\alpha_m$) are penalized in the final sum.
\end{itemize}

Question : where are these $G_m$'s coming from? Are they pre-set or are they created by the algorithm?

\section{Computational Learning Theory}
\subsection{Definitions}
\begin{itemize}
\item $H$--hypothesis space. $c \in H$--true hypothesis. $h \in H$--candidate hypothesis. $S \subseteq H$--training set.
\item Consistent learner: Learner outputs a hypothesis such that $h(x) = c(x)$ $\forall x \in S$
\item Version space: $VS(S) = \{ h \in H: $h$ \text{ consistent wrt to } S \}$ (ie, hypothesis consistent with training examples)
\item training error: fraction of training examples misclassified by $h$.
\item true error: fraction of examples that would be misclassified on sample drawn from $D$ (distribution over inputs). $error_D(h) = Pr_{x \sim D} [c(x) \neq h(x)]$
\item $C$ is PAC-learnable by learner $L$ using $H$ $\iff$ $L$ will output $h \in H$ (with probability $1-\delta$) such that $error_D(h) \leq \varepsilon$ in time and samples polynomial in $1/\varepsilon$, $1/ \delta$, $|H|$.
\item $\varepsilon$-exhausted version space: $VS(S)$ exhausted iff $\forall h \in VS(S)$ $error_D(h) \leq \varepsilon$.
\end{itemize}

\subsection{Haussler Theorem}
Bounds true error.\\
Let $error_D(h_i) > \varepsilon$ for $i = 1 \dots k$ (some $h_i$'s in $H$). How much data do we need to ``knock out'' all these hypotheses?\\
$Pr_{x \sim D} [h_i(x) = c(x)] \leq 1- \varepsilon$ (probability that $h_i$ matches true concept is low)\\
$Pr(h_i \text{ consistent with $c$ on $m$ examples}) \leq (1- \varepsilon)^m$ (independent).\\
$Pr(\exists h_i \text{ consistent with $c$ on $m$ examples}) = k \cdot (1- \varepsilon)^m \leq |H| \cdot (1-\varepsilon)^m$\\
$-\varepsilon \geq ln(1- \varepsilon) \implies (1- \varepsilon)^m \leq exp(-\varepsilon m)$\\
Upper bound that VS not $\varepsilon$-exhausted after $m$ samples: $|H|\cdot exp(-\varepsilon m)$.\\
Want: $|H| \cdot exp(-\varepsilon m) \leq \delta$ (solve for m).\\
$m \geq \frac{1}{\varepsilon} (ln(|H|) + ln (\frac{1}{\delta})$

\subsection{Infinite Hypotheses Spaces}
\begin{itemize}
\item Examples: linear separators, ANNs, decision trees (continuous inputs)
\item $m \geq \frac{1}{\varepsilon} (8VC(H)lg(\frac{13}{\varepsilon}) + 4 lg( \frac{2}{\delta})$
\item shatter: A set of instances $S$ is shattered by $H$ if every possible dichotomy of $S$ $\exists h \in H$ that is consistent with this dichotomy.
\item $VC(H)$ is size of largest finite subset of instance space that can be shattered by $H$.
\item $C$ PAC-learnable iff VC dimension is finite.
\end{itemize}


\section{Bayesian Learning}
\subsection{Equations and Definitions}
\begin{itemize}
\item $P(h)$ : probability that a hypothesis $h$ holds
\item $P(D)$: probability that training data $D$ will be observed
\item Bayes' Rule: $$P(h|D) = \frac{P(D|h)P(h)}{P(D)}$$
\item Find most probable $h \in H$ given $D$: $$h_{map} = argmax_{h \in H} P(h|D) = argmax_{h \in H} P(D|h)P(h)$$
\item if every $h\in H$ a priori equally probable: $$h_{ml} = argmax_{h \in H} P(D|h)$$
\end{itemize}

\subparagraph{BRUTE FORCE MAP learning algorithm}
Output $h_{map}$

Let's assume:
\begin{itemize}
\item $D$ is noise-free
\item Target function $c \in H$
\item all $h$ (a priori) are equally likely
\end{itemize}
Then $P(h) = \frac1{|H|}$\\
$$P(D|h) =
    \begin{cases}
            1, &         \text{if } d_i =h(x_i) \forall d_i \in D,\\
            0, &         \text{otherwise}.
    \end{cases}
$$
$$P(D) = \frac{|VS_{H,D}|}{|H|}$$ $|VS_{H,D}|$ is the set of hypotheses in $H$ that are consistent with $D$. Consistent learned outputs an $h$ with zero error over training examples.\\
Therefore $$P(h|D) = \begin{cases}
            \frac1{|VS_{H,D}|}, &         \text{if $h$ consistent with $D$}\\
            0, &         \text{otherwise}.
    \end{cases}
$$
Every consistent hypothesis is a MAP hypothesis (with these assumptions)!

\subsection{ML and Least-Squared Error}
Under certain assumptions any learner that minimizes squared error between the outputs of hypothesis $h$ and training data will output an ML hypothesis. No idea why. ?? ML hypothesis is the one that minimizes the sum of squared errors over the training data.

\subsection{Bayes Optimal Classifier}
$$P(v_j |D) = \sum_{h_j \in H}{P(v_j|h_i) P(h_i|D)}$$ (probability that correct classification is $v_j$)\\
$$v_{map} = argmax_{v_j \in V} P(v_j|D)$$

\subsection{Bayesian Belief Networks}
\subparagraph{Naive Bayes} Classify given attributes: $v_{map} = argmax_{v_j \in V} P(v_j|a_1,...,a_n)$. Rewrite using Bayes' rule and use naive assumption that all $a_i$ are conditionally independent given $v_j$. $v_{NB} = argmax_{v_j \in V} P(v_j) \prod_{i}{P(a_i|v_j)}$.\\
Whenever naive assumption is satisfied, $v_{NB}$ same as MAP classification.
\subparagraph{EM Algorithm}
\begin{itemize}
\item arbitrary initial hypothesis
\item repeatedly calculates expected values of the hidden variables
\item recalculates the ML hypothesis
\end{itemize}
This will converge to local ML hypothesis, along with estimated values for hidden variables (why?)

\section{Evaluating Hypotheses}

\section{Randomized Optimization}
\subsection{MIMIC}
Directly model distribution.\\
Algorithm:\\
\begin{itemize}
\item generate samples from $P^{\theta_t}(x)$
\item set $\theta_{t+1}$ to the n'th percentile
\item retain only those samples such that $f(x) \geq \theta_{t+1}$
\item estimate $P^{\theta_{t+1}}(x)$
\item repeat!
\end{itemize}

\subsection{Simulated Annealing}
Algorithm:\\
\begin{itemize}
\item for finite number of iterations:
\item sample new point $x_t$ in $N(x)$
\item Jump to new sample with probability $P(x, x_t, T)$
\item decrease $T$
\end{itemize}
$$P(x, x_t, T) =
    \begin{cases}
            1, &         \text{if } f(x_t) \geq f(x),\\
            exp(\frac{f(x_t)-f(x)}{T}), &         \text{otherwise}.
    \end{cases}
$$

\section{Information Theory}
\subparagraph{Definitions}
We'll use shorthand: Just write $x$ instead of $X= x$ for all the possible values that a random event $X$ could take on. (Am I using the terms correctly?)
\begin{itemize}
\item Mutual Information: $I(X,Y) = H(X) - H(X|Y)$
\item Entropy: $H(A) = - \sum_{s \in A} P(s) lg(P(s))$
\item Joint entropy: $H(X, Y) = - \sum_{x \in X} \sum_{y \in Y} P(x,y) lg(P(x,y))$
\item Conditional Entropy: $H(Y|X) = -\sum_{x \in X} \sum_{y \in Y} P(x,y) lg(P(y|x))$
\item If X independent of Y: $H(Y|X) = H(Y)$ and $H(Y,X) = H(Y) +H(X)$
\item Kullback-Leibler divergence: $KL(p||q) = - \sum_{x \in X} p(x) lg(\frac{p(x)}{q(x)})$ for two different distributions $p$, $q$.
\end{itemize}

\end{document}